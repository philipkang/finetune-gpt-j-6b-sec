# finetune-gpt-j-6b with public SEC filing of Amazon (2021-2022)

This project is demonstrate the enhanced fine-tuning process of a SageMaker JumpStart Foundation model (gpt-j-6b) with custom domain knowledge dataset of a publicly available SEC filing of AMAZON from 2021 to 2022.  It is surprisingly just a couple lines of code, you can finetune a JumpStart Foundation Model.

1. Set Up
2. Select Text Generation Model GPT-J 6B
3. Fine-tune the pre-trained model on a custom dataset
   a. Set Training parameters
   b. Start Training   
5. Deploy & run inference
   
