{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39f1d60-9f9d-4fcc-94cf-655be6768db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.11/site-packages (2.237.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.35.85)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.115.6)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<2.3,>=2.2 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.17)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.32.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.85 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.35.85)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (0.10.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.21.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.11/site-packages (from omegaconf<2.3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.10.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.22.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (2024.8.30)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker) (0.41.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->sagemaker) (4.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.70.17)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from starlette<0.42.0,>=0.40.0->fastapi->sagemaker) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d433dd-d51d-48df-80ef-5ae896bf15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"huggingface-textgeneration1-gpt-j-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe395402-7fd1-41ac-92e6-3100624e18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.jumpstart.utils import get_jumpstart_content_bucket\n",
    "\n",
    "# Sample training data is available in this bucket\n",
    "data_bucket = get_jumpstart_content_bucket()\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36955507-8fea-49df-b820-add810003cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpstart-cache-prod-us-east-1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a155b6-4435-49ab-b05b-33b77df81f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-textgeneration1-gpt-j-6b' with wildcard version identifier '*'. You can pin to version '3.2.4' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 01:10:10] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Using model <span style=\"color: #008700; text-decoration-color: #008700\">'huggingface-textgeneration1-gpt-j-6b'</span> with wildcard version  <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/cache.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cache.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/cache.py#625\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">625</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         identifier <span style=\"color: #008700; text-decoration-color: #008700\">'*'</span>. You can pin to version <span style=\"color: #008700; text-decoration-color: #008700\">'3.2.4'</span> for more stable results.   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Note that models may have different input/output signatures after a major <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version upgrade.                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 01:10:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Using model \u001b[38;2;0;135;0m'huggingface-textgeneration1-gpt-j-6b'\u001b[0m with wildcard version  \u001b]8;id=337366;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/cache.py\u001b\\\u001b[2mcache.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=582362;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/cache.py#625\u001b\\\u001b[2m625\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         identifier \u001b[38;2;0;135;0m'*'\u001b[0m. You can pin to version \u001b[38;2;0;135;0m'3.2.4'\u001b[0m for more stable results.   \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Note that models may have different input/output signatures after a major \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         version upgrade.                                                          \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for training job. Defaulting to ml.g5.12xlarge.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 01:10:11] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> No instance type selected for training job. Defaulting to             <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/estimator.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">estimator.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/estimator.py#530\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ml.g5.12xlarge.                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 01:10:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m No instance type selected for training job. Defaulting to             \u001b]8;id=481844;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/estimator.py\u001b\\\u001b[2mestimator.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=950095;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/estimator.py#530\u001b\\\u001b[2m530\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ml.g5.12xlarge.                                                       \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters={\"epoch\": \"3\", \"per_device_train_batch_size\": \"4\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4957a43-b136-45c3-b628-910e4725bb6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 01:10:49] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#90\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 01:10:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=48270;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=734658;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 01:10:50] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-01-10-49-953                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 01:10:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=492961;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=197854;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-01-10-49-953                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 01:10:50 Starting - Starting the training job\n",
      "2024-12-23 01:10:50 Pending - Training job waiting for capacity......\n",
      "2024-12-23 01:11:39 Pending - Preparing the instances for training...\n",
      "2024-12-23 01:12:12 Downloading - Downloading input data..........................................................................................\n",
      "2024-12-23 01:27:12 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:15,900 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:15,937 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:15,947 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:15,949 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:17,151 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.23.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.12.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.10.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.5.0-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.2-py2.py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.12.0->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.23.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.23.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0->-r requirements.txt (line 4)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0->-r requirements.txt (line 4)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.12.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907844 sha256=ad71b0248b66b1969f12accea54e9441a73cecd2b930fbfec8866897975b068a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b4/a0/a9/4723ccba9b5790d90f40617f369a69c6dff729fa4b0aa6e131\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, deepspeed, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 datasets-2.12.0 deepspeed-0.10.3 peft-0.5.0 safetensors-0.3.3 sagemaker-jumpstart-huggingface-script-utilities-1.1.2 sagemaker-jumpstart-script-utilities-1.1.8 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,098 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,098 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,154 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,202 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,250 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,261 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"bf16\": \"False\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epoch\": \"3\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"20\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": \"True\",\n",
      "        \"gradient_accumulation_steps\": \"2\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"instruction_tuned\": \"False\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"6e-06\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"10\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"8\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": \"1\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"train_from_scratch\": \"False\",\n",
      "        \"validation_split_ratio\": \"0.2\",\n",
      "        \"warmup_ratio\": \"0.1\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"hf-textgeneration1-gpt-j-6b-2024-12-23-01-10-49-953\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"hf-textgeneration1-gpt-j-6b-2024-12-23-01-10-49-953\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"False\",\"--bf16\",\"False\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epoch\",\"3\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"20\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--instruction_tuned\",\"False\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"6e-06\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"-1\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--train_from_scratch\",\"False\",\"--validation_split_ratio\",\"0.2\",\"--warmup_ratio\",\"0.1\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=True\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=False\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=6e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FROM_SCRATCH=False\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --bf16 False --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 3 --eval_accumulation_steps None --eval_steps 20 --evaluation_strategy steps --fp16 True --gradient_accumulation_steps 2 --gradient_checkpointing True --instruction_tuned False --label_smoothing_factor 0 --learning_rate 6e-06 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 10 --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_input_length -1 --max_steps -1 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 8 --per_device_train_batch_size 4 --preprocessing_num_workers None --save_steps 500 --save_strategy steps --save_total_limit 1 --seed 10 --train_data_split_seed 0 --train_from_scratch False --validation_split_ratio 0.2 --warmup_ratio 0.1 --warmup_steps 0 --weight_decay 0.2\u001b[0m\n",
      "\u001b[34m2024-12-23 01:27:34,292 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(model_dir='/opt/ml/model', train=None, train_alt='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', hosts=['algo-1'], num_gpus=4, current_host='algo-1', pretrained_model='/opt/ml/input/data/model', peft_type='None', lora_r=64, lora_alpha=16, lora_dropout=0.0, bits=16, double_quant=True, quant_type='nf4', deepspeed=True, instruction_tuned='False', train_from_scratch='False', fp16='True', bf16='False', evaluation_strategy='steps', eval_steps=20, epoch=3, gradient_accumulation_steps=2, per_device_train_batch_size=4, per_device_eval_batch_size=8, logging_steps=10, warmup_ratio=0.1, learning_rate=6e-06, weight_decay=0.2, load_best_model_at_end='True', max_train_samples=-1, max_val_samples=-1, seed=10, max_input_length=-1, validation_split_ratio=0.2, train_data_split_seed=0, preprocessing_num_workers=None, max_steps=-1, gradient_checkpointing='True', early_stopping_patience=3, early_stopping_threshold=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_nan_inf_filter='True', save_strategy='steps', save_steps=500, save_total_limit=1, dataloader_drop_last='False', dataloader_num_workers=0, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_steps=0).\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /tmp. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Parameter 'instruction_tuned' is identified as 'False'. Domain adaption fine-tuning will be started.\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:26,512] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:28,569] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:28,570] [INFO] [runner.py:570:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py --deepspeed ds_config.json --model_name_or_path /tmp --train_file /opt/ml/input/data/train --do_train --output_dir /opt/ml/model --num_train_epochs 3 --gradient_accumulation_steps 2 --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --logging_steps 10 --warmup_ratio 0.1 --learning_rate 6e-06 --weight_decay 0.2 --seed 10 --max_input_length -1 --validation_split_ratio 0.2 --train_data_split_seed 0 --max_steps -1 --early_stopping_patience 3 --early_stopping_threshold 0.0 --adam_beta1 0.9 --adam_beta2 0.999 --max_grad_norm 1.0 --label_smoothing_factor 0.0 --logging_strategy steps --save_strategy steps --save_steps 500 --dataloader_num_workers 0 --lr_scheduler_type constant_with_warmup --warmup_steps 0 --evaluation_strategy steps --eval_steps 20 --lora_r 64 --lora_alpha 16 --lora_dropout 0.0 --bits 16 --quant_type nf4 --validation_file /opt/ml/input/data/validation --load_best_model_at_end --fp16 --gradient_checkpointing --save_total_limit 1 --double_quant\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:30,207] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:163:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:32,175] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:37,221] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:37,228] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:37,232] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:37,236] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,368] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,368] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,380] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,406] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,410] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m12/23/2024 01:30:39 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m12/23/2024 01:30:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=6e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Dec23_01-30-36_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant_with_warmup,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.2,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m12/23/2024 01:30:39 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m12/23/2024 01:30:39 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m12/23/2024 01:30:39 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,437 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-12-23 01:30:39,438 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2024-12-23 01:30:39,496 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2024-12-23 01:30:39,496 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2024-12-23 01:30:39,512 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2024-12-23 01:30:39,512 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2024-12-23 01:30:39,528 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2024-12-23 01:30:39,528 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,529] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,529] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1176] 2024-12-23 01:30:39,529 >> Instantiating GPTJForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1176] 2024-12-23 01:30:39,529 >> Instantiating GPTJForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2024-12-23 01:30:39,529 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2024-12-23 01:30:39,529 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,529] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2024-12-23 01:30:39,533 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2024-12-23 01:30:39,533 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:39,534] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:30:45,571] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 285, num_elems = 6.05B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.86s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.94s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2024-12-23 01:31:12,437 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2024-12-23 01:31:12,437 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2024-12-23 01:31:12,437 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2024-12-23 01:31:12,437 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2024-12-23 01:31:12,440 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2024-12-23 01:31:12,440 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.21s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.21s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 13888.42it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2054.52it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 716.42it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 712.29it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 706.77it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 674.05it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:12 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   6%|▌         | 6000/106367 [00:00<00:02, 49543.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 44477.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 42915.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 43968.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█▏        | 12000/106367 [00:00<00:01, 52501.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█▏        | 12000/106367 [00:00<00:01, 52430.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█▏        | 12000/106367 [00:00<00:01, 50693.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:01, 49925.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 18000/106367 [00:00<00:01, 54449.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 19000/106367 [00:00<00:01, 54377.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 18000/106367 [00:00<00:01, 50617.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 49452.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 53181.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 50604.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|██▎       | 25000/106367 [00:00<00:01, 52338.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 23000/106367 [00:00<00:01, 50599.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  29%|██▉       | 31000/106367 [00:00<00:01, 41585.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  29%|██▉       | 31000/106367 [00:00<00:01, 40726.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 29000/106367 [00:00<00:01, 40149.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  31%|███       | 33000/106367 [00:00<00:01, 41322.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 38000/106367 [00:00<00:01, 46872.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  35%|███▍      | 37000/106367 [00:00<00:01, 43531.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|███▎      | 35000/106367 [00:00<00:01, 43201.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 40000/106367 [00:00<00:01, 45220.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  41%|████▏     | 44000/106367 [00:00<00:01, 47560.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  40%|████      | 43000/106367 [00:00<00:01, 45341.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▊      | 41000/106367 [00:00<00:01, 44828.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|████▎     | 46000/106367 [00:00<00:01, 46203.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|████▋     | 50000/106367 [00:01<00:01, 47841.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  46%|████▌     | 49000/106367 [00:01<00:01, 46341.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  45%|████▌     | 48000/106367 [00:01<00:01, 48873.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|████▉     | 52000/106367 [00:01<00:01, 47177.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  53%|█████▎    | 56000/106367 [00:01<00:01, 48094.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 55000/106367 [00:01<00:01, 47442.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  51%|█████     | 54000/106367 [00:01<00:01, 49050.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  55%|█████▍    | 58000/106367 [00:01<00:01, 37795.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 39223.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 40167.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|█████▋    | 61000/106367 [00:01<00:01, 40728.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  62%|██████▏   | 66000/106367 [00:01<00:00, 45158.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  65%|██████▍   | 69000/106367 [00:01<00:00, 43884.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 68000/106367 [00:01<00:00, 42920.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  63%|██████▎   | 67000/106367 [00:01<00:00, 43315.81 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 72000/106367 [00:01<00:00, 46905.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 76000/106367 [00:01<00:00, 47214.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  70%|██████▉   | 74000/106367 [00:01<00:00, 45412.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  69%|██████▊   | 73000/106367 [00:01<00:00, 46147.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  74%|███████▍  | 79000/106367 [00:01<00:00, 50505.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  78%|███████▊  | 83000/106367 [00:01<00:00, 49329.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▌  | 80000/106367 [00:01<00:00, 46114.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  74%|███████▍  | 79000/106367 [00:01<00:00, 48618.67 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▍ | 90000/106367 [00:01<00:00, 52331.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  82%|████████▏ | 87000/106367 [00:01<00:00, 49593.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 85000/106367 [00:01<00:00, 40610.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 85000/106367 [00:01<00:00, 40863.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  87%|████████▋ | 93000/106367 [00:02<00:00, 46510.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  86%|████████▋ | 92000/106367 [00:02<00:00, 44974.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  92%|█████████▏| 98000/106367 [00:02<00:00, 42665.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  88%|████████▊ | 94000/106367 [00:02<00:00, 41490.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  93%|█████████▎| 99000/106367 [00:02<00:00, 46225.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  92%|█████████▏| 98000/106367 [00:02<00:00, 45172.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 104000/106367 [00:02<00:00, 43074.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  94%|█████████▍| 100000/106367 [00:02<00:00, 42241.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|█████████▊| 105000/106367 [00:02<00:00, 48086.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 104000/106367 [00:02<00:00, 46465.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|█████████▉| 106000/106367 [00:02<00:00, 44225.65 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  37%|███▋      | 7000/18760 [00:00<00:00, 61743.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  37%|███▋      | 7000/18760 [00:00<00:00, 58746.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  37%|███▋      | 7000/18760 [00:00<00:00, 58786.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 55906.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▍  | 14000/18760 [00:00<00:00, 60902.31 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  69%|██████▉   | 13000/18760 [00:00<00:00, 56068.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 12000/18760 [00:00<00:00, 36627.73 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▌ | 16000/18760 [00:00<00:00, 39458.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12414.99 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 18760/18760 [00:00<00:00, 46437.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12501.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12967.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14120.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12979.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14064.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14377.08 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14754.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14409.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14663.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14946.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15095.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14960.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 14915.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15169.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15479.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15181.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15252.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15527.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15422.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15511.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15167.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15733.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15433.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15417.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15436.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16123.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15723.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15703.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 15795.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16054.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15691.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16068.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15382.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15619.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16317.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15644.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 15990.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16222.07 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 16028.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16228.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15722.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15941.67 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15952.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:06, 13316.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16357.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16362.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:06, 13096.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:05, 13873.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16086.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16081.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:05, 13628.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:05, 14634.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16308.45 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16301.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:05, 14395.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:02<00:05, 15028.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16216.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16213.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:02<00:05, 14748.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 15195.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16014.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 16001.84 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 15967.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 14930.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16573.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16547.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 15670.24 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 15826.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16222.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16200.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16519.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 15511.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16792.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16768.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16172.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16358.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16527.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16501.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16534.35 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:04, 16036.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16649.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16622.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16218.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16295.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16360.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16334.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:03<00:03, 16179.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 15988.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16211.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16185.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16079.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:03<00:03, 15864.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16083.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 16070.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15728.23 examples/s]#015Grouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 15780.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15712.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15698.77 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15442.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15168.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15132.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15108.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15869.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 14892.24 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15822.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15780.65 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15787.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15588.39 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15741.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15694.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15509.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15988.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15945.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15900.46 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15706.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15714.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15651.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15622.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15622.73 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:04<00:02, 15416.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15567.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15542.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15615.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15340.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15543.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15525.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15693.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15316.35 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15618.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15608.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15808.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15356.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15752.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15741.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15482.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15490.27 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15431.73 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15421.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 16117.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 16081.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15184.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 16051.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15568.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 15805.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15546.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15520.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15277.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15848.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 15804.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:04<00:01, 15786.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15566.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15558.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15526.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15517.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 16044.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15988.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15273.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15997.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15972.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15921.82 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15740.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15931.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15972.84 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15930.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15666.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15902.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15849.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15823.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15678.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15778.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15806.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15768.27 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15564.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15736.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15997.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15950.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15522.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15913.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15752.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:05<00:00, 15709.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:06<00:00, 15704.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15668.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16424.67 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16364.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15479.51 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16330.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 15007.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16124.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14954.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14913.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14261.85 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14723.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14219.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14151.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14155.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14111.26 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14002.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14045.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14537.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14501.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 13902.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14426.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14270.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14183.75 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14145.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14038.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14238.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14217.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 13918.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14112.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14486.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14430.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 13978.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14339.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14519.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14454.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14204.08 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14361.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14550.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14480.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14231.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14404.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14602.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14547.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14281.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14478.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14513.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14459.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14351.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14387.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14357.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14291.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14274.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14211.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14507.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14429.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14114.90 examples/s]\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a852e5cfb3d4478d.arrow\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,588] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4817a2d6c4018d5a.arrow\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14342.49 examples/s]\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,635] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a852e5cfb3d4478d.arrow\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4817a2d6c4018d5a.arrow\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,706] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14251.70 examples/s]\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a852e5cfb3d4478d.arrow\u001b[0m\n",
      "\u001b[34m12/23/2024 01:31:23 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-397de98d3b9da839/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4817a2d6c4018d5a.arrow\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2024-12-23 01:31:23,810 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2024-12-23 01:31:23,810 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,814] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:23,823] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:24,963] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:24,964] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:24,964] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:24,974] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.438207626342773 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.540225505828857 seconds\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.541991233825684 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.543391227722168 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000006, betas=(0.900000, 0.999000), weight_decay=0.200000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,572] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,587] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,587] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,587] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,587] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,695] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,696] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.89 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,696] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.84 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,698] [INFO] [stage3.py:126:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,698] [INFO] [stage3.py:127:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,805] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,806] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,806] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.84 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 811008 in 114 params\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,931] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,932] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:56,932] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.84 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:57,042] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:57,042] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:31:57,042] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.84 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,126] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,143] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,143] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 46.87 GB, percent = 25.1%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,405] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,406] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:01,406] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 51.78 GB, percent = 27.7%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,272] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,273] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,273] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.94 GB, percent = 34.8%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,405] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,405] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:03,406] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.53 GB, percent = 36.2%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:11,635] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:11,636] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:11,636] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 133.11 GB, percent = 71.3%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:11,636] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,920] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,920] [INFO] [utils.py:804:see_memory_usage] MA 0.15 GB         Max_MA 0.92 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,921] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 144.5 GB, percent = 77.4%\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f414c1f3fd0>\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[6e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,922] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,922] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,922] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,922] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,922] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f41b01084c0>\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,923] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   optimizer_params ............. {'lr': 6e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.2}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,924] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 6e-06, 'warmup_num_steps': 10}\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  4\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-12-23 01:32:14,925] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 6e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.2\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 6e-06, \n",
      "            \"warmup_num_steps\": 10\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2024-12-23 01:32:14,926 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2024-12-23 01:32:14,926 >>   Num examples = 1,030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2024-12-23 01:32:14,926 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2024-12-23 01:32:14,926 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2024-12-23 01:32:14,927 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2024-12-23 01:32:14,927 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2024-12-23 01:32:14,927 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2024-12-23 01:32:14,926 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2024-12-23 01:32:14,926 >>   Num examples = 1,030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2024-12-23 01:32:14,926 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2024-12-23 01:32:14,926 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2024-12-23 01:32:14,927 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2024-12-23 01:32:14,927 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2024-12-23 01:32:14,927 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2024-12-23 01:32:14,928 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2024-12-23 01:32:14,928 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m0%|          | 0/96 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/96 [00:40<1:03:31, 40.12s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/96 [01:14<58:00, 37.02s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/96 [01:49<55:50, 36.03s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/96 [02:24<54:27, 35.52s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/96 [03:00<53:59, 35.59s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 6/96 [03:34<52:39, 35.10s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/96 [04:08<51:33, 34.76s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 8/96 [04:42<50:31, 34.45s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 9/96 [05:16<50:00, 34.49s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [05:50<49:01, 34.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6971, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [05:50<49:01, 34.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 11/96 [06:24<48:31, 34.25s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 12/96 [06:58<47:54, 34.22s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 13/96 [07:32<47:13, 34.14s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 14/96 [08:07<46:51, 34.28s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 15/96 [08:41<46:12, 34.23s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 16/96 [09:16<45:42, 34.28s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 17/96 [09:49<44:57, 34.14s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 18/96 [10:24<44:30, 34.24s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 19/96 [10:57<43:40, 34.03s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:31<42:57, 33.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3914, 'learning_rate': 6e-06, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:31<42:57, 33.91s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 01:43:46,420 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 01:43:46,420 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 01:43:46,424 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 01:43:46,424 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 01:43:46,425 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 01:43:46,425 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:09,  2.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.93s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.2705078125, 'eval_runtime': 26.1273, 'eval_samples_per_second': 7.578, 'eval_steps_per_second': 0.268, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:57<42:57, 33.91s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 21/96 [12:31<52:22, 41.90s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 22/96 [13:06<48:46, 39.55s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 23/96 [13:41<46:31, 38.24s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 24/96 [14:14<44:00, 36.68s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 25/96 [14:47<42:12, 35.67s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 26/96 [15:21<41:09, 35.28s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 27/96 [15:56<40:14, 35.00s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 28/96 [16:29<39:02, 34.45s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 29/96 [17:03<38:11, 34.21s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 30/96 [17:36<37:26, 34.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1307, 'learning_rate': 6e-06, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 30/96 [17:36<37:26, 34.03s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 31/96 [18:10<36:50, 34.01s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 32/96 [18:44<36:16, 34.02s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 33/96 [19:16<35:07, 33.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 34/96 [19:51<34:48, 33.68s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 35/96 [20:24<34:11, 33.63s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 36/96 [20:58<33:50, 33.83s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 37/96 [21:32<33:16, 33.83s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 38/96 [22:07<33:00, 34.15s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 39/96 [22:41<32:26, 34.15s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:15<31:45, 34.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8149, 'learning_rate': 6e-06, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:15<31:45, 34.03s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 01:55:30,438 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 01:55:30,438 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 01:55:30,438 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 01:55:30,438 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 01:55:30,438 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 01:55:30,438 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.84s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.39s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.78076171875, 'eval_runtime': 25.787, 'eval_samples_per_second': 7.678, 'eval_steps_per_second': 0.271, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:41<31:45, 34.03s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:22<00:00,  3.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 41/96 [24:15<38:21, 41.85s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 42/96 [24:50<35:41, 39.65s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 43/96 [25:23<33:26, 37.86s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 44/96 [25:57<31:44, 36.63s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 45/96 [26:31<30:31, 35.91s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 46/96 [27:05<29:20, 35.22s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 47/96 [27:39<28:29, 34.89s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 48/96 [28:13<27:41, 34.62s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 49/96 [28:47<26:55, 34.37s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [29:21<26:16, 34.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6572, 'learning_rate': 6e-06, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [29:21<26:16, 34.27s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 51/96 [29:55<25:38, 34.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 52/96 [30:29<24:59, 34.09s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 53/96 [31:02<24:13, 33.80s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 54/96 [31:35<23:34, 33.67s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 55/96 [32:08<22:55, 33.55s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 56/96 [32:43<22:28, 33.72s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 57/96 [33:16<21:51, 33.63s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 58/96 [33:50<21:17, 33.62s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 59/96 [34:23<20:45, 33.66s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [34:57<20:13, 33.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5648, 'learning_rate': 6e-06, 'epoch': 1.85}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [34:57<20:13, 33.71s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:07:12,644 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:07:12,644 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:07:12,644 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:07:12,644 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:07:12,644 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:07:12,644 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.84s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.00s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.23s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.53955078125, 'eval_runtime': 25.7859, 'eval_samples_per_second': 7.679, 'eval_steps_per_second': 0.271, 'epoch': 1.85}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [35:23<20:13, 33.71s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:22<00:00,  3.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 61/96 [35:56<24:04, 41.26s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 62/96 [36:30<22:07, 39.06s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 63/96 [37:03<20:30, 37.29s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 64/96 [37:37<19:17, 36.17s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 65/96 [38:10<18:10, 35.18s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 66/96 [38:42<17:15, 34.51s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 67/96 [39:16<16:36, 34.35s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 68/96 [39:50<15:51, 33.99s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 69/96 [40:24<15:19, 34.07s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 70/96 [40:58<14:46, 34.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4614, 'learning_rate': 6e-06, 'epoch': 2.15}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 70/96 [40:58<14:46, 34.09s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 71/96 [41:32<14:08, 33.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 72/96 [42:05<13:31, 33.81s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 73/96 [42:39<12:54, 33.70s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 74/96 [43:12<12:19, 33.61s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 75/96 [43:46<11:47, 33.69s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 76/96 [44:20<11:15, 33.77s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 77/96 [44:54<10:41, 33.76s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 78/96 [45:27<10:04, 33.59s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 79/96 [46:01<09:33, 33.72s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:34<08:55, 33.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3456, 'learning_rate': 6e-06, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:34<08:55, 33.46s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:18:49,060 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:18:49,060 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:18:49,063 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:18:49,063 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:18:49,063 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:18:49,063 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.84s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.24s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.48s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.43408203125, 'eval_runtime': 25.8124, 'eval_samples_per_second': 7.671, 'eval_steps_per_second': 0.271, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:59<08:55, 33.46s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:22<00:00,  3.48s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 81/96 [47:33<10:18, 41.22s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 82/96 [48:06<09:04, 38.88s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 83/96 [48:39<08:02, 37.09s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 84/96 [49:13<07:12, 36.00s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 85/96 [49:47<06:30, 35.50s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 86/96 [50:21<05:50, 35.09s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 87/96 [50:55<05:13, 34.78s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 88/96 [51:29<04:35, 34.41s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 89/96 [52:03<04:01, 34.50s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [52:37<03:25, 34.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3468, 'learning_rate': 6e-06, 'epoch': 2.77}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [52:37<03:25, 34.23s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 91/96 [53:10<02:49, 33.96s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 92/96 [53:44<02:15, 33.90s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 93/96 [54:18<01:41, 33.92s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 94/96 [54:52<01:07, 33.83s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 95/96 [55:26<00:33, 33.88s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:59<00:00, 33.79s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2024-12-23 02:28:14,798 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2024-12-23 02:28:14,798 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 3359.8721, 'train_samples_per_second': 0.92, 'train_steps_per_second': 0.029, 'train_loss': 0.79360032081604, 'epoch': 2.95}\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:59<00:00, 33.79s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:59<00:00, 35.00s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       2.95\u001b[0m\n",
      "\u001b[34mtrain_loss               =     0.7936\u001b[0m\n",
      "\u001b[34mtrain_runtime            = 0:55:59.87\u001b[0m\n",
      "\u001b[34mtrain_samples            =       1030\u001b[0m\n",
      "\u001b[34mtrain_samples_per_second =       0.92\u001b[0m\n",
      "\u001b[34mtrain_steps_per_second   =      0.029\u001b[0m\n",
      "\u001b[34m12/23/2024 02:28:14 - INFO - __main__ -   Start Evaluation.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:28:14,821 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-12-23 02:28:14,821 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:28:14,825 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-12-23 02:28:14,825 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:28:14,826 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-12-23 02:28:14,826 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.82s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.60s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.99s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.22s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.37s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.17s/it]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\n",
      "  epoch                   =       2.95\n",
      "  eval_loss               =     0.3635\n",
      "  eval_runtime            = 0:00:25.67\n",
      "  eval_samples            =        198\n",
      "  eval_samples_per_second =      7.712\n",
      "  eval_steps_per_second   =      0.273\n",
      "  perplexity              =     1.4384\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2024-12-23 02:28:40,498 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2024-12-23 02:28:40,498 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2024-12-23 02:28:40,499 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2024-12-23 02:28:40,499 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2024-12-23 02:28:40,500 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2024-12-23 02:28:40,500 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2024-12-23 02:28:40,679 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2024-12-23 02:28:40,679 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2024-12-23 02:28:40,680 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2024-12-23 02:28:40,680 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2024-12-23 02:28:40,680 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2024-12-23 02:28:40,680 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:28:53,007] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step96 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:28:53,008] [INFO] [engine.py:3486:save_16bit_model] Saving model weights to /opt/ml/model/pytorch_model.bin, tag: global_step96\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:28:53,008] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:28:58,829] [INFO] [launch.py:347:main] Process 140 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:28:59,830] [INFO] [launch.py:347:main] Process 138 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:29:00,831] [INFO] [launch.py:347:main] Process 139 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:29:15,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:29:15,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step96 is ready now!\u001b[0m\n",
      "\u001b[34m[2024-12-23 02:29:23,855] [INFO] [launch.py:347:main] Process 137 exits successfully.\u001b[0m\n",
      "\n",
      "2024-12-23 02:30:01 Uploading - Uploading generated training model\u001b[34mINFO:root:Convert: [2/2] -- Took: 0:00:27.021591 s\u001b[0m\n",
      "\u001b[34m2024-12-23 02:29:56,110 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-12-23 02:29:56,110 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-12-23 02:29:56,110 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-12-23 02:30:59 Completed - Training job completed\n",
      "Training seconds: 4727\n",
      "Billable seconds: 4727\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b95b2d-bae6-43d4-8b9e-8af4251fd545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 06:11:25] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> No instance type selected for inference hosting endpoint. Defaulting to   <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/model.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/model.py#238\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ml.g5.2xlarge.                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 06:11:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m No instance type selected for inference hosting endpoint. Defaulting to   \u001b]8;id=472928;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/model.py\u001b\\\u001b[2mmodel.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=619019;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/factory/model.py#238\u001b\\\u001b[2m238\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ml.g5.2xlarge.                                                            \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name:                                              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-06-11-25-701                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name:                                              \u001b]8;id=286128;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=233598;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-06-11-25-701                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 06:11:26] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5889</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-06-11-25-696                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 06:11:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=58979;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=727314;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5889\u001b\\\u001b[2m5889\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-06-11-25-696                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4711\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4711</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-06-11-25-696                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=745304;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=578992;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4711\u001b\\\u001b[2m4711\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-06-11-25-696                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4eecf5b-5d8c-4da7-900b-d93fc024830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \" we have not filed, or understated,accrued interest and penalties on past-due payroll and other taxes. We arecooperating with the IRS in their investigation.  /s/ Ernst & Young LLPWe have audited Amazon.com, Inc.'s internal control over financial reportingas of December 31, 2006, based on criteria established in Internal Control-Integrated Framework issued by the Committee of Sponsoring Organizations of the Treadway Commission (the COSO criteria). Amazon.com, Inc.'s management isresponsible for maintaining effective internal control over financialreporting, and for its assessment of the effectiveness of internal control overfinancial reporting as of December 31, 2006. Our responsibility is to express anopinion on management's assessment and an opinion on the effectiveness of thecompany's internal control over financial reporting based on our audit.We conducted our audit in accordance with the standards of the Public CompanyAccounting Oversight Board (United States). Those standards require that weplan and perform the audit to obtain reasonable assurance about whethereffective internal control over financial reporting was maintained in allmaterial respects. Our audit included obtaining an understanding of internalcontrol over financial reporting, assessing the risk that a material weaknessexists, testing and evaluating the design and operating effectiveness ofinternal control based on the assessed risk, and performing such otherprocedures as we considered necessary in the circumstances. We believe thatour audit provides a reasonable basis for our opinion.A company's internal control over financial reporting is a process designed toprovide reasonable assurance regarding the reliability of financial reportingand the preparation of financial statements for external purposes inaccordance with generally accepted accounting principles. A company's internalcontrol over financial reporting includes those policies and procedures that(1) pertain to the maintenance of records that, in reasonable detail,accurately and fairly reflect the transactions and dispositions of the assetsof the company; (2) provide reasonable assurance that transactions arerecorded as necessary to permit preparation of financial statements inaccordance\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {\"inputs\": \"This Form 10-K report shows that\", \"parameters\": {\"max_new_tokens\": 400}}\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190629f2-0a23-4f37-971c-977f8d615aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/23/24 07:33:05] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4865\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4865</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-06-11-25-696                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/23/24 07:33:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=684804;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=798849;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4865\u001b\\\u001b[2m4865\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-06-11-25-696                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name:                                           <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4855\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4855</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hf-textgeneration1-gpt-j-6b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-12-23-06-11-25-696                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name:                                           \u001b]8;id=733300;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=431945;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4855\u001b\\\u001b[2m4855\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hf-textgeneration1-gpt-j-6b-\u001b[1;36m2024\u001b[0m-12-23-06-11-25-696                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor.delete_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e7355-40cb-4361-bf06-e66570012872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
